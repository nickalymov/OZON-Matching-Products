{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:54:09.636505300Z",
     "start_time": "2024-09-08T06:54:08.108764900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import Levenshtein\n",
    "from scipy.stats import entropy\n",
    "import re\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "attributes_path = './data/test/attributes_test.parquet'\n",
    "resnet_path = './data/test/resnet_test.parquet'\n",
    "text_and_bert_path = './data/test/text_and_bert_test.parquet'\n",
    "val_path = './data/test/test.parquet'\n",
    "\n",
    "attributes = pd.read_parquet(attributes_path, engine='pyarrow')\n",
    "attributes.set_index('variantid', inplace=True)\n",
    "resnet = pd.read_parquet(resnet_path, engine='pyarrow')\n",
    "resnet.set_index('variantid', inplace=True)\n",
    "text = pd.read_parquet(text_and_bert_path, engine='pyarrow')\n",
    "text.set_index('variantid', inplace=True)\n",
    "test = pd.read_parquet(val_path, engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:56:24.529371100Z",
     "start_time": "2024-09-08T06:54:09.639499700Z"
    }
   },
   "id": "2a87c3ae0c1af2c4",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('my_data/unique_cats2.pkl', 'rb') as f:\n",
    "    unique_cats2 = pickle.load(f)\n",
    "\n",
    "with open('my_data/unique_cats3.pkl', 'rb') as f:\n",
    "    unique_cats3 = pickle.load(f)\n",
    "\n",
    "with open('my_data/minor_keys.pkl', 'rb') as f:\n",
    "    minor_keys = pickle.load(f)\n",
    "\n",
    "with open('my_data/vital_keys.pkl', 'rb') as f:\n",
    "    vital_keys = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:57:12.161552700Z",
     "start_time": "2024-09-08T06:57:12.112982600Z"
    }
   },
   "id": "2b4129629025bfce",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # attributes\n",
    "    'cat_fit': True, # Количество одинаковых категорий / 4 \n",
    "    'cat2': True, # Категориальный признак 2 категории\n",
    "    'cat3': True, # Категориальный признак 3 категории\n",
    "    'jac_attrs': True, # Сходство Жаккара для аттрибутов (только ключей) \n",
    "    'jacm_attrs': True,\n",
    "    'com_attrs': True, # Количество общих атрибутов\n",
    "    'jac_vals': True, # Среднее сходств Жаккара для значений (для общих ключей)\n",
    "    'com_vals': True, # Количество одинаковых значений общих атрибутов\n",
    "    'jac_num_vals': True, # Сходство Жаккара для числовых общих значений\n",
    "    'rat_num_vals': True, # Среднее процентное отношение значений общих атрибутов\n",
    "    'jac_sev_vals': True, # Сходство Жаккара для общих значений в ключах (значений в ключе > 1)\n",
    "    'jac_vital_vals':True, # Сходство значений значимых ключей (по статистике выше)\n",
    "    'jac_minor_vals':True, # Сходство незначений значимых ключей (по статистике выше)\n",
    "    'diff_attrs': True, # Разность количеств аттрибутов (ключей) / max(len(attrs1), len(attrs2))\n",
    "    \n",
    "    # text\n",
    "    'n_len_diff': True, # Разность количеств символов имен / max(len(name1), len(name2))\n",
    "    'd_len_diff': True, # Разность количеств символов описаний / max(len(desc1), len(desc2))\n",
    "    'n_lev': True, # Расстояние Левенштейна между именами / max(len(name1), len(name2))\n",
    "    'd_lev': True, # Расстояние Левенштейна между описаниями / max(len(desc1), len(desc2))\n",
    "    'n_jac_symbs': True, # Жаккарово сравнение символов имен\n",
    "    'd_jac_symbs': True, # Жаккарово сравнение символов описаний\n",
    "    \n",
    "    'n_jac': True, # Жаккарово сравнение имен по 6 вариантам токенизации\n",
    "    'd_jac': True, # Жаккарово сравнение описаний по 6 вариантам токенизации\n",
    "    'n_lev_opers': True, # Расстояние Левенштейна между именами / max(len(name1), len(name2))\n",
    "    'd_lev_opers': True, # Расстояние Левенштейна между описаниями / max(len(desc1), len(desc2))\n",
    "    \n",
    "    # resnet\n",
    "    'm_cos': True, # Косинусное сходство между основными эмбеддингами\n",
    "    'm_evklid': True, # Евклидово расстояние между основными эмбеддингами\n",
    "    'e_jac': True, # Жаккарово сравнение дополнительных эмбеддингов\n",
    "    'e_diff': True, # Разница в количестве дополнительных эмбеддингов\n",
    "    'm_ent_diff': True, # Энтропия (не знаю что за функция) основных эмбеддингов\n",
    "    'e_avg_cos': True, # Косинусное сходство между средним эмбеддингом дополнительных эмбеддингов\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:57:13.534010800Z",
     "start_time": "2024-09-08T06:57:13.513010400Z"
    }
   },
   "id": "b5d211bd70255fab",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_texts(sentence):\n",
    "    if sentence is None:\n",
    "        return None\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence) \n",
    "    return sentence\n",
    "text['name'] = text['name'].apply(preprocess_texts)\n",
    "text['description'] = text['description'].apply(preprocess_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:58:34.354259Z",
     "start_time": "2024-09-08T06:57:13.854200400Z"
    }
   },
   "id": "a9aa8009de86f17e",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 42350.72it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('cat_fit', False):\n",
    "    def cat_fit(test_df, attributes_df):\n",
    "        \n",
    "        def compute_cat_fit(variantid1, variantid2):\n",
    "            \n",
    "            cat1 = json.loads(attributes_df.at[variantid1, 'categories'])\n",
    "            cat2 = json.loads(attributes_df.at[variantid2, 'categories'])\n",
    "                \n",
    "            common_keys = set(cat1.keys()) & set(cat2.keys())\n",
    "            \n",
    "            return (sum(1 for key in common_keys if cat1[key] == cat2[key])) / len(common_keys)\n",
    "            \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['cat_fit'] = test_df.progress_apply(lambda row: compute_cat_fit(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "    \n",
    "    test = cat_fit(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:58:37.063076100Z",
     "start_time": "2024-09-08T06:58:35.836415400Z"
    }
   },
   "id": "f9470cddf4da7376",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:00<00:00, 67994.45it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('cat2', False):\n",
    "    def cat2(test_df, attributes_df):\n",
    "        \n",
    "        def compute_cat2(variantid1, variantid2, u_cats=unique_cats2):\n",
    "            \n",
    "            cat1 = json.loads(attributes_df.at[variantid1, 'categories'])\n",
    "            cat2 = json.loads(attributes_df.at[variantid2, 'categories'])\n",
    "            \n",
    "            if cat1['2'] == cat2['2']:\n",
    "                if cat1['2'] in u_cats:\n",
    "                    return u_cats[cat1['2']]\n",
    "                else:\n",
    "                    return -2\n",
    "            else:\n",
    "                return -1\n",
    "            \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['cat2'] = test_df.progress_apply(lambda row: compute_cat2(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = cat2(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:58:38.193861100Z",
     "start_time": "2024-09-08T06:58:37.447191Z"
    }
   },
   "id": "c735213275b59049",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:00<00:00, 68574.37it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('cat3', False):\n",
    "    def cat3(test_df, attributes_df):\n",
    "        \n",
    "        def compute_cat3(variantid1, variantid2, u_cats=unique_cats3):\n",
    "            \n",
    "            cat1 = json.loads(attributes_df.at[variantid1, 'categories'])\n",
    "            cat2 = json.loads(attributes_df.at[variantid2, 'categories'])\n",
    "            \n",
    "            if cat1['3'] == cat2['3']:\n",
    "                if cat1['3'] in u_cats:\n",
    "                    return u_cats[cat1['3']]\n",
    "                else:\n",
    "                    return -2\n",
    "            else:\n",
    "                return -1\n",
    "            \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['cat3'] = test_df.progress_apply(lambda row: compute_cat3(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = cat3(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:58:39.293463700Z",
     "start_time": "2024-09-08T06:58:38.546933500Z"
    }
   },
   "id": "b12cc97b196a79b5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:09<00:00, 5225.91it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('jac_attrs', False):\n",
    "    def jac_attrs(test_df, attributes_df):\n",
    "\n",
    "        def compute_jac_attrs(variantid1, variantid2):\n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "\n",
    "            keys1 = set(attrs1.keys())\n",
    "            keys2 = set(attrs2.keys())\n",
    "\n",
    "            intersection = len(keys1.intersection(keys2))\n",
    "            union = len(keys1.union(keys2))\n",
    "\n",
    "            if intersection == 0:\n",
    "                return -2\n",
    "\n",
    "            return intersection / union if union != 0 else 0\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['jac_attrs'] = test_df.progress_apply(\n",
    "            lambda row: compute_jac_attrs(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "\n",
    "    test = jac_attrs(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:58:49.993568700Z",
     "start_time": "2024-09-08T06:58:40.487891900Z"
    }
   },
   "id": "951ea046e1edf652",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 40108.59it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('jacm_attrs', False):\n",
    "    def jac_attrs(test_df, attributes_df):\n",
    "        \n",
    "        def compute_jac_attrs(variantid1, variantid2):\n",
    "            \n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            keys1 = set(attrs1.keys())\n",
    "            keys2 = set(attrs2.keys())\n",
    "\n",
    "            # Количество общих элементов (пересечение множеств)\n",
    "            intersection = len(keys1.intersection(keys2))\n",
    "            \n",
    "            # Доля общих элементов относительно множества 1\n",
    "            ratio1 = intersection / len(keys1)\n",
    "            \n",
    "            # Доля общих элементов относительно множества 2\n",
    "            ratio2 = intersection / len(keys2)\n",
    "            \n",
    "            # Возвращаем максимальное значение из двух долей\n",
    "            return max(ratio1, ratio2)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['jacm_attrs'] = test_df.progress_apply(lambda row: compute_jac_attrs(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = jac_attrs(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:58:51.248710100Z",
     "start_time": "2024-09-08T06:58:49.994569100Z"
    }
   },
   "id": "6bec38763dbbb0c2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 41794.92it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('com_attrs', False):\n",
    "    def com_attrs(test_df, attributes_df):\n",
    "        def compute_com_attrs(variantid1, variantid2):\n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "\n",
    "            keys1 = set(attrs1.keys())\n",
    "            keys2 = set(attrs2.keys())\n",
    "\n",
    "            return len(keys1 & keys2)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['com_attrs'] = test_df.progress_apply(\n",
    "            lambda row: compute_com_attrs(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "\n",
    "    test = com_attrs(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:02.923470400Z",
     "start_time": "2024-09-08T06:59:01.724998200Z"
    }
   },
   "id": "5738bd56efd61717",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 31132.70it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('jac_vals', False):\n",
    "    def jac_vals(test_df, attributes_df):\n",
    "        \n",
    "        def compute_jac_vals(variantid1, variantid2):\n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            \n",
    "            common_keys = set(attrs1.keys()).intersection(set(attrs2.keys()))\n",
    "\n",
    "            if not common_keys:\n",
    "                return -2  \n",
    "\n",
    "            jaccard_scores = []\n",
    "\n",
    "            for key in common_keys:\n",
    "                set_values1 = set(attrs1[key])\n",
    "                set_values2 = set(attrs2[key])\n",
    "    \n",
    "                intersection = len(set_values1.intersection(set_values2))\n",
    "                union = len(set_values1.union(set_values2))\n",
    "                \n",
    "                jaccard_score = intersection / union if union != 0 else 0\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "            \n",
    "            return sum(jaccard_scores) / len(jaccard_scores)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['jac_vals'] = test_df.progress_apply(lambda row: compute_jac_vals(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = jac_vals(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:04.832594700Z",
     "start_time": "2024-09-08T06:59:03.225573800Z"
    }
   },
   "id": "44ddde5d86d29d04",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 35865.90it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('com_vals', False):    \n",
    "    def com_vals(test_df, attributes_df):\n",
    "        \n",
    "        def compute_com_vals(variantid1, variantid2):\n",
    "            \n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            \n",
    "            common_keys = set(attrs1.keys()).intersection(set(attrs2.keys()))\n",
    "    \n",
    "            if not common_keys:\n",
    "                return -2  \n",
    "    \n",
    "            scores = 0\n",
    "    \n",
    "            for key in common_keys:\n",
    "                set_values1 = set(attrs1[key])\n",
    "                set_values2 = set(attrs2[key])\n",
    "    \n",
    "                if set_values1 == set_values2:\n",
    "                    scores += 1\n",
    "            \n",
    "            return scores\n",
    "        \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['com_vals'] = test_df.progress_apply(lambda row: compute_com_vals(row['variantid1'], row['variantid2']), axis=1)\n",
    "    \n",
    "        return test_df\n",
    "    \n",
    "    test = com_vals(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:06.233789900Z",
     "start_time": "2024-09-08T06:59:04.830584800Z"
    }
   },
   "id": "879f17efee072f96",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 26453.30it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('jac_num_vals', False):\n",
    "    def jac_num_vals(test_df, attributes_df):\n",
    "        \n",
    "        def compute_jac_num_vals(variantid1, variantid2):\n",
    "            \n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            \n",
    "            common_keys = set(attrs1.keys()).intersection(set(attrs2.keys()))\n",
    "            \n",
    "            if not common_keys:\n",
    "                return -2  \n",
    "            \n",
    "            num_common_keys = set()\n",
    "            \n",
    "            for key in common_keys:\n",
    "                if len(attrs1[key]) == len(attrs2[key]):\n",
    "                    digit = True\n",
    "                    for val1, val2 in zip(attrs1[key], attrs2[key]):\n",
    "                        if bool(re.match(r'^[-+]?\\d*\\.?\\d+([eE][-+]?\\d+)?$', val1)) and bool(re.match(r'^[-+]?\\d*\\.?\\d+([eE][-+]?\\d+)?$', val1)):\n",
    "                            continue\n",
    "                        else:\n",
    "                            digit = False\n",
    "                    if digit:\n",
    "                        num_common_keys.add(key)\n",
    "            \n",
    "            if len(num_common_keys) == 0:\n",
    "                return -3\n",
    "            \n",
    "            jaccard_scores = []\n",
    "\n",
    "            for key in num_common_keys:\n",
    "                set_values1 = set(attrs1[key])\n",
    "                set_values2 = set(attrs2[key])\n",
    "    \n",
    "                intersection = len(set_values1.intersection(set_values2))\n",
    "                union = len(set_values1.union(set_values2))\n",
    "                \n",
    "                jaccard_score = intersection / union if union != 0 else 0\n",
    "                \n",
    "                jaccard_scores.append(jaccard_score)\n",
    "            \n",
    "            return sum(jaccard_scores) / len(jaccard_scores)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['jac_num_vals'] = test_df.progress_apply(\n",
    "            lambda row: compute_jac_num_vals(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = jac_num_vals(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:08.122562700Z",
     "start_time": "2024-09-08T06:59:06.231881900Z"
    }
   },
   "id": "e3b3465353e59b76",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 25525.73it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('rat_num_vals', False):    \n",
    "    def rat_num_vals(test_df, attributes_df):\n",
    "        \n",
    "        def compute_rat_num_vals(variantid1, variantid2):\n",
    "            \n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            \n",
    "            common_keys = set(attrs1.keys()).intersection(set(attrs2.keys()))\n",
    "            \n",
    "            if not common_keys:\n",
    "                return -2  \n",
    "            \n",
    "            num_common_keys = set()\n",
    "            \n",
    "            for key in common_keys:\n",
    "                if len(attrs1[key]) == len(attrs2[key]):\n",
    "                    digit = True\n",
    "                    for val1, val2 in zip(attrs1[key], attrs2[key]):\n",
    "                        if bool(re.match(r'^[-+]?\\d*\\.?\\d+([eE][-+]?\\d+)?$', val1)) and bool(re.match(r'^[-+]?\\d*\\.?\\d+([eE][-+]?\\d+)?$', val2)):\n",
    "                            continue\n",
    "                        else:\n",
    "                            digit = False\n",
    "                    if digit:\n",
    "                        num_common_keys.add(key)\n",
    "            \n",
    "            if len(num_common_keys) == 0:\n",
    "                return -3\n",
    "            \n",
    "            rat_scores = []\n",
    "    \n",
    "            for key in num_common_keys:\n",
    "                set_values1 = set(attrs1[key])\n",
    "                set_values2 = set(attrs2[key])\n",
    "                \n",
    "                if len(set_values1) == 1 and len(set_values2) == 1:\n",
    "                    val1 = float(list(set_values1)[0])\n",
    "                    val2 = float(list(set_values2)[0])\n",
    "                    \n",
    "                    if val1 == val2:\n",
    "                        rat_scores.append(1.0)  # Значения одинаковы\n",
    "                    else:\n",
    "                        # Используем абсолютное значение разности\n",
    "                        abs_diff = abs(val1 - val2)\n",
    "                        # Нормируем разницу, чтобы она находилась в диапазоне от 0 до 1\n",
    "                        # В этом случае устанавливаем максимальное значение для нормализации.\n",
    "                        max_diff = max(abs(val1), abs(val2))\n",
    "                        normalized_diff = 1 - (abs_diff / (max_diff if max_diff != 0 else 1))\n",
    "                        rat_scores.append(normalized_diff)\n",
    "                \n",
    "                else:\n",
    "                    intersection = len(set_values1.intersection(set_values2))\n",
    "                    union = len(set_values1.union(set_values2))\n",
    "                    \n",
    "                    jaccard_score = intersection / union if union != 0 else 0\n",
    "                    \n",
    "                    rat_scores.append(jaccard_score)\n",
    "            \n",
    "            return sum(rat_scores) / len(rat_scores)\n",
    "    \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['rat_num_vals'] = test_df.progress_apply(\n",
    "            lambda row: compute_rat_num_vals(row['variantid1'], row['variantid2']), axis=1)\n",
    "    \n",
    "        return test_df\n",
    "    \n",
    "    test = rat_num_vals(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:10.087500900Z",
     "start_time": "2024-09-08T06:59:08.127643Z"
    }
   },
   "id": "40f3b57d03394927",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 37502.33it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('jac_sev_vals', False):\n",
    "    def jac_sev_vals(test_df, attributes_df):\n",
    "        \n",
    "        def compute_jac_sev_vals(variantid1, variantid2):\n",
    "            \n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            \n",
    "            common_keys = set(attrs1.keys()).intersection(set(attrs2.keys()))\n",
    "            \n",
    "            if not common_keys:\n",
    "                return -2  \n",
    "            \n",
    "            sev_common_keys = set()\n",
    "            \n",
    "            for key in common_keys:\n",
    "                if len(attrs1[key]) > 1 or len(attrs2[key]) > 1:\n",
    "                        sev_common_keys.add(key)\n",
    "            \n",
    "            if len(sev_common_keys) == 0:\n",
    "                return -3\n",
    "            \n",
    "            jaccard_scores = []\n",
    "\n",
    "            for key in sev_common_keys:\n",
    "                set_values1 = set(attrs1[key])\n",
    "                set_values2 = set(attrs2[key])\n",
    "    \n",
    "                intersection = len(set_values1.intersection(set_values2))\n",
    "                union = len(set_values1.union(set_values2))\n",
    "                \n",
    "                jaccard_score = intersection / union if union != 0 else 0\n",
    "                \n",
    "                jaccard_scores.append(jaccard_score)\n",
    "            \n",
    "            return sum(jaccard_scores) / len(jaccard_scores)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['jac_sev_vals'] = test_df.progress_apply(\n",
    "            lambda row: compute_jac_sev_vals(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = jac_sev_vals(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:11.427651200Z",
     "start_time": "2024-09-08T06:59:10.088489800Z"
    }
   },
   "id": "987e767e6208c867",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 30649.64it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('jac_vital_vals', False):\n",
    "    def jac_vital_vals(test_df, attributes_df):\n",
    "        \n",
    "        def compute_jac_vital_vals(variantid1, variantid2, v_keys=vital_keys):\n",
    "            \n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            \n",
    "            common_keys = set(attrs1.keys()).intersection(set(attrs2.keys()))\n",
    "\n",
    "            if not common_keys:\n",
    "                return -2  \n",
    "            \n",
    "            common_vital_keys = set()\n",
    "            \n",
    "            for key in common_keys:\n",
    "                if key in v_keys:\n",
    "                    common_vital_keys.add(key)\n",
    "            \n",
    "            if len(common_vital_keys) == 0:\n",
    "                return -3\n",
    "                    \n",
    "            jaccard_scores = []\n",
    "\n",
    "            for key in common_vital_keys:\n",
    "                set_values1 = set(attrs1[key])\n",
    "                set_values2 = set(attrs2[key])\n",
    "    \n",
    "                intersection = len(set_values1.intersection(set_values2))\n",
    "                union = len(set_values1.union(set_values2))\n",
    "                \n",
    "                jaccard_score = intersection / union if union != 0 else 0\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "            \n",
    "            return sum(jaccard_scores) / len(jaccard_scores)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['jac_vital_vals'] = test_df.progress_apply(\n",
    "            lambda row: compute_jac_vital_vals(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = jac_vital_vals(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:13.060596200Z",
     "start_time": "2024-09-08T06:59:11.431575300Z"
    }
   },
   "id": "eee2c2195be90e0d",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 30278.51it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('jac_minor_vals', False):\n",
    "    def jac_minor_vals(test_df, attributes_df):\n",
    "        \n",
    "        def compute_jac_minor_vals(variantid1, variantid2, m_keys=minor_keys):\n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            \n",
    "            common_keys = set(attrs1.keys()).intersection(set(attrs2.keys()))\n",
    "\n",
    "            if not common_keys:\n",
    "                return -2  \n",
    "            \n",
    "            common_minor_keys = set()\n",
    "            \n",
    "            for key in common_keys:\n",
    "                if key in m_keys:\n",
    "                    common_minor_keys.add(key)\n",
    "            \n",
    "            if len(common_minor_keys) == 0:\n",
    "                return -3\n",
    "                    \n",
    "            jaccard_scores = []\n",
    "\n",
    "            for key in common_minor_keys:\n",
    "                set_values1 = set(attrs1[key])\n",
    "                set_values2 = set(attrs2[key])\n",
    "    \n",
    "                intersection = len(set_values1.intersection(set_values2))\n",
    "                union = len(set_values1.union(set_values2))\n",
    "                \n",
    "                jaccard_score = intersection / union if union != 0 else 0\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "            \n",
    "            return sum(jaccard_scores) / len(jaccard_scores)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['jac_minor_vals'] = test_df.progress_apply(lambda row: compute_jac_minor_vals(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    test = jac_minor_vals(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:14.748617100Z",
     "start_time": "2024-09-08T06:59:13.063596300Z"
    }
   },
   "id": "5286aa94a54ca492",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:02<00:00, 22540.36it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('diff_attrs', False):\n",
    "    def diff_attrs(test_df, attributes_df):\n",
    "        \n",
    "        def compute_diff_attrs(variantid1, variantid2):\n",
    "            attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "            attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "            \n",
    "            return (abs(len(attrs1) - len(attrs2))) / max(len(attrs1), len(attrs2))\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['diff_attrs'] = test_df.progress_apply(lambda row: compute_diff_attrs(row['variantid1'], row['variantid2']), axis=1)\n",
    "\n",
    "        return test_df\n",
    "    \n",
    "    test = diff_attrs(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:16.968137900Z",
     "start_time": "2024-09-08T06:59:14.740522600Z"
    }
   },
   "id": "dd75a1eb84648d90",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 32396.58it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('n_len_diff', False):\n",
    "    def n_len_diff(test_df, text_df):\n",
    "        \n",
    "        def compute_n_len_diff(variantid1, variantid2):\n",
    "            \n",
    "            name1 = text_df.at[variantid1, 'name']\n",
    "            name2 = text_df.at[variantid2, 'name']\n",
    "                \n",
    "            if len(name1) < 3  or len(name2) < 3 :\n",
    "                if len(name1) < 3  and len(name2) < 3 :\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            return abs(len(name1) - len(name2)) / max(len(name1), len(name2))\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['n_len_diff'] = test_df.progress_apply(lambda row: compute_n_len_diff(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "\n",
    "    test = n_len_diff(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:18.511942500Z",
     "start_time": "2024-09-08T06:59:16.964639300Z"
    }
   },
   "id": "3233a2c43c847a71",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 29691.89it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('d_len_diff', False):\n",
    "    def d_len_diff(test_df, text_df):\n",
    "        \n",
    "        def compute_d_len_diff(variantid1, variantid2):\n",
    "            \n",
    "            desc1 = text_df.at[variantid1, 'description']\n",
    "            desc2 = text_df.at[variantid2, 'description']\n",
    "            \n",
    "            if desc1 is None or desc2 is None:\n",
    "                if desc1 is None and desc2 is None:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "                \n",
    "            if len(desc1.replace(' ', '')) == 0  or len(desc2.replace(' ', '')) == 0:\n",
    "                if len(desc1.replace(' ', '')) == 0  and len(desc2.replace(' ', '')) == 0:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "                \n",
    "            return abs(len(desc1) - len(desc2)) / max(len(desc1), len(desc2))\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['d_len_diff'] = test_df.progress_apply(lambda row: compute_d_len_diff(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "\n",
    "    test = d_len_diff(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:20.232218100Z",
     "start_time": "2024-09-08T06:59:18.517648700Z"
    }
   },
   "id": "951bed0a5a567f28",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 35445.17it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('n_lev', False):\n",
    "    def n_lev(test_df, text_df):\n",
    "        \n",
    "        def compute_n_lev(variantid1, variantid2):\n",
    "            \n",
    "            name1 = text_df.at[variantid1, 'name']\n",
    "            name2 = text_df.at[variantid2, 'name']\n",
    "\n",
    "            if len(name1) < 3  or len(name2) < 3 :\n",
    "                if len(name1) < 3  and len(name2) < 3 :\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "                \n",
    "            return Levenshtein.distance(name1, name2) / max(len(name1), len(name2))\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['n_lev'] = test_df.progress_apply(lambda row: compute_n_lev(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = n_lev(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:21.630019700Z",
     "start_time": "2024-09-08T06:59:20.217175100Z"
    }
   },
   "id": "5870b62c0e83dd9f",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:03<00:00, 12633.10it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('d_lev', False):\n",
    "    def d_lev(test_df, text_df):\n",
    "        \n",
    "        def compute_d_lev(variantid1, variantid2):\n",
    "            \n",
    "            desc1 = text_df.at[variantid1, 'description']\n",
    "            desc2 = text_df.at[variantid2, 'description']\n",
    "            \n",
    "            if desc1 is None or desc2 is None:\n",
    "                if desc1 is None and desc2 is None:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            if len(desc1.replace(' ', '')) == 0  or len(desc2.replace(' ', '')) == 0:\n",
    "                if len(desc1.replace(' ', '')) == 0  and len(desc2.replace(' ', '')) == 0:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "                \n",
    "            return Levenshtein.distance(desc1, desc2) / max(len(desc1), len(desc2))\n",
    "    \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['d_lev'] = test_df.progress_apply(lambda row: compute_d_lev(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = d_lev(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:25.594409800Z",
     "start_time": "2024-09-08T06:59:21.633020100Z"
    }
   },
   "id": "3d12cec328ea44b5",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:01<00:00, 26548.22it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('n_jac_symbs', False):\n",
    "    def n_jac_symbs(test_df, text_df):\n",
    "        \n",
    "        def compute_n_jac_symbs(variantid1, variantid2):\n",
    "            \n",
    "            symbols1 = set(text_df.at[variantid1, 'name'])\n",
    "            symbols2 = set(text_df.at[variantid2, 'name'])\n",
    "            \n",
    "            if len(symbols1) == 0 or len(symbols2) == 0:\n",
    "                if len(symbols1) == 0 and len(symbols2) == 0:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            intersection = len(symbols1.intersection(symbols2))\n",
    "            union = len(symbols1.union(symbols2))\n",
    "            \n",
    "            return intersection / union if union != 0 else 0\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['n_jac_symbs'] = test_df.progress_apply(lambda row: compute_n_jac_symbs(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = n_jac_symbs(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:27.481567500Z",
     "start_time": "2024-09-08T06:59:25.582393500Z"
    }
   },
   "id": "800c4baad011c18c",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:04<00:00, 10770.15it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('d_jac_symbs', False):\n",
    "    def d_jac_symbs(test_df, text_df):\n",
    "        \n",
    "        def compute_d_jac_symbs(variantid1, variantid2):\n",
    "            \n",
    "            desc1 = text_df.at[variantid1, 'description']\n",
    "            desc2 = text_df.at[variantid2, 'description']\n",
    "            \n",
    "            if desc1 is None or desc2 is None:\n",
    "                if desc1 is None and desc2 is None:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            symbols1 = set(desc1)\n",
    "            symbols2 = set(desc2)\n",
    "            \n",
    "            if len(symbols1) == 0 or len(symbols2) == 0:\n",
    "                if len(symbols1) == 0 and len(symbols2) == 0: \n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            intersection = len(symbols1.intersection(symbols2))\n",
    "            union = len(symbols1.union(symbols2))\n",
    "            \n",
    "            return intersection / union if union != 0 else 0\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['d_jac_symbs'] = test_df.progress_apply(lambda row: compute_d_jac_symbs(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = d_jac_symbs(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:32.109099Z",
     "start_time": "2024-09-08T06:59:27.473565200Z"
    }
   },
   "id": "f7ed82f5ad957669",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 49620/49620 [00:14<00:00, 3410.28it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('n_jac', False):\n",
    "    def n_jac(test_df, text_df):\n",
    "        \n",
    "        def create_bigrams(sentence):\n",
    "            bigrams = []\n",
    "            for i in range(len(sentence) - 1):\n",
    "                bigrams.append((sentence[i], sentence[i + 1]))\n",
    "            return bigrams\n",
    "        \n",
    "        def filter_bigrams(bigrams):\n",
    "            return [(w1, w2) for (w1, w2) in bigrams if re.match(r'\\d+', w1)]\n",
    "        \n",
    "        def compute_jac_value(tokens1, tokens2):\n",
    "            \n",
    "            if len(tokens1) == 0  or len(tokens2) == 0 :\n",
    "                if len(tokens1) == 0  and len(tokens2) == 0 :\n",
    "                    return -4\n",
    "                else:\n",
    "                    return -3\n",
    "            \n",
    "            intersection = len(tokens1.intersection(tokens2))\n",
    "            union = len(tokens1.union(tokens2))\n",
    "            \n",
    "            return intersection / union if union > 0 else 0\n",
    "        \n",
    "        def compute_jacm_value(tokens1, tokens2):\n",
    "            \n",
    "            if len(tokens1) == 0  or len(tokens2) == 0 :\n",
    "                if len(tokens1) == 0  and len(tokens2) == 0 :\n",
    "                    return -4\n",
    "                else:\n",
    "                    return -3\n",
    "            \n",
    "            intersection = len(tokens1.intersection(tokens2))\n",
    "            # Доля общих элементов относительно множества 1\n",
    "            ratio1 = intersection / len(tokens1)\n",
    "            \n",
    "            # Доля общих элементов относительно множества 2\n",
    "            ratio2 = intersection / len(tokens2)\n",
    "            \n",
    "            # Возвращаем максимальное значение из двух долей\n",
    "            return max(ratio1, ratio2)\n",
    "        \n",
    "        def compute_n_jac(variantid1, variantid2):\n",
    "            \n",
    "            name1 = text_df.at[variantid1, 'name']\n",
    "            name2 = text_df.at[variantid2, 'name']\n",
    "            \n",
    "            if name1 is None:\n",
    "                name1 = ''\n",
    "            if name2 is None:\n",
    "                name2 = ''\n",
    "            \n",
    "            if len(name1) < 3  or len(name2) < 3 :\n",
    "                if len(name1) < 3  and len(name2) < 3 :\n",
    "                    return -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2\n",
    "                else:\n",
    "                    return -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n",
    "                \n",
    "            # Токены без пунктуации\n",
    "            words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            words1 = words1.replace('_', ' ') # Удаление _ символов\n",
    "            words1 = re.sub(r'\\s+', ' ', words1) # Удаление лишних пробелов\n",
    "            words1 = words1.strip().split() # Удаление пробелов в начале и конце    \n",
    "            #\n",
    "            words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            words2 = words2.replace('_', ' ') # Удаление _ символов\n",
    "            words2 = re.sub(r'\\s+', ' ', words2) # Удаление лишних пробелов\n",
    "            words2 = words2.strip().split() # Удаление пробелов в начале и конце    \n",
    "            #\n",
    "            n_jac_words = compute_jac_value(set(words1), set(words2))\n",
    "            n_jacm_words = compute_jacm_value(set(words1), set(words2))\n",
    "            \n",
    "            \n",
    "            ru_words1 = re.sub(r'[^а-яА-Я]', ' ', name1) # Получение токенов с ру символами\n",
    "            ru_words1 = re.sub(r'\\s+', ' ', ru_words1) # Удаление лишних пробелов\n",
    "            ru_words1 = ru_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            ru_words2 = re.sub(r'[^а-яА-Я]', ' ', name2) # Получение токенов с ру символами\n",
    "            ru_words2 = re.sub(r'\\s+', ' ', ru_words2) # Удаление лишних пробелов\n",
    "            ru_words2 = ru_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_ru_words = compute_jac_value(set(ru_words1), set(ru_words2))\n",
    "            n_jacm_ru_words = compute_jacm_value(set(ru_words1), set(ru_words2))\n",
    "            \n",
    "            \n",
    "            en_words1 = re.sub(r'[^a-zA-Z]', ' ', name1) # Получение токенов с ру символами\n",
    "            en_words1 = re.sub(r'\\s+', ' ', en_words1) # Удаление лишних пробелов\n",
    "            en_words1 = en_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            en_words2 = re.sub(r'[^a-zA-Z]', ' ', name2) # Получение токенов с ру символами\n",
    "            en_words2 = re.sub(r'\\s+', ' ', en_words2) # Удаление лишних пробелов\n",
    "            en_words2 = en_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_en_words = compute_jac_value(set(en_words1), set(en_words2))\n",
    "            n_jacm_en_words = compute_jacm_value(set(en_words1), set(en_words2))\n",
    "            \n",
    "            \n",
    "            ru_comb_words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            ru_comb_words1 = ru_comb_words1.replace('_', ' ') # Удаление _ символов\n",
    "            ru_comb_words1 = ' '.join(re.findall(r'\\b(?=\\w*[а-яА-Я])(?=\\w*[^\\Wа-яА-Я])\\w+\\b', ru_comb_words1))\n",
    "            ru_comb_words1 = re.sub(r'\\s+', ' ', ru_comb_words1) # Удаление лишних пробелов\n",
    "            ru_comb_words1 = ru_comb_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            ru_comb_words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            ru_comb_words2 = ru_comb_words2.replace('_', ' ') # Удаление _ символов\n",
    "            ru_comb_words2 = ' '.join(re.findall(r'\\b(?=\\w*[а-яА-Я])(?=\\w*[^\\Wа-яА-Я])\\w+\\b', ru_comb_words2))\n",
    "            ru_comb_words2 = re.sub(r'\\s+', ' ', ru_comb_words2) # Удаление лишних пробелов\n",
    "            ru_comb_words2 = ru_comb_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_ru_comb_words = compute_jac_value(set(ru_comb_words1), set(ru_comb_words2))\n",
    "            n_jacm_ru_comb_words = compute_jacm_value(set(ru_comb_words1), set(ru_comb_words2))\n",
    "            \n",
    "            \n",
    "            en_comb_words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            en_comb_words1 = en_comb_words1.replace('_', ' ') # Удаление _ символов\n",
    "            en_comb_words1 = ' '.join(re.findall(r'\\b(?=\\w*[a-zA-Z])(?=\\w*[^\\Wa-zA-Z])\\w+\\b', en_comb_words1))\n",
    "            en_comb_words1 = re.sub(r'\\s+', ' ', en_comb_words1) # Удаление лишних пробелов\n",
    "            en_comb_words1 = en_comb_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            en_comb_words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            en_comb_words2 = en_comb_words2.replace('_', ' ') # Удаление _ символов\n",
    "            en_comb_words2 = ' '.join(re.findall(r'\\b(?=\\w*[a-zA-Z])(?=\\w*[^\\Wa-zA-Z])\\w+\\b', en_comb_words2))\n",
    "            en_comb_words2 = re.sub(r'\\s+', ' ', en_comb_words2) # Удаление лишних пробелов\n",
    "            en_comb_words2 = en_comb_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_en_comb_words = compute_jac_value(set(en_comb_words1), set(en_comb_words2))\n",
    "            n_jacm_en_comb_words = compute_jacm_value(set(en_comb_words1), set(en_comb_words2))\n",
    "            \n",
    "            \n",
    "            numbers1 = re.sub(r'[^\\d]', ' ', name1) # Получение токенов с цифрами\n",
    "            numbers1 = re.sub(r'\\s+', ' ', numbers1) # Удаление лишних пробелов\n",
    "            numbers1 = numbers1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            numbers2 = re.sub(r'[^\\d]', ' ', name2) # Получение токенов с цифрами\n",
    "            numbers2 = re.sub(r'\\s+', ' ', numbers2) # Удаление лишних пробелов\n",
    "            numbers2 = numbers2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_numbers = compute_jac_value(set(numbers1), set(numbers2))\n",
    "            n_jacm_numbers = compute_jacm_value(set(numbers1), set(numbers2))\n",
    "            \n",
    "        \n",
    "            bigrams1 = create_bigrams(words1)\n",
    "            bigrams2 = create_bigrams(words2)\n",
    "            #\n",
    "            filtered_bigrams1 = filter_bigrams(bigrams1)\n",
    "            filtered_bigrams2 = filter_bigrams(bigrams2)\n",
    "            #\n",
    "            length = 0\n",
    "            counter = 0\n",
    "            #\n",
    "            for w1, w2 in filtered_bigrams1:\n",
    "                for x1, x2 in filtered_bigrams2:\n",
    "                    if w2 == x2:\n",
    "                        length += 1\n",
    "                        if w1 == x1:\n",
    "                            counter += 1\n",
    "            #\n",
    "            if length != 0:\n",
    "                num_words = counter / length\n",
    "            else:\n",
    "                num_words = -3\n",
    "            \n",
    "            words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            words1 = words1.replace('_', ' ') # Удаление _ символов\n",
    "            words1 = ' '.join(re.findall(r'\\b\\d{4}\\b', words1))\n",
    "            words1 = re.sub(r'\\s+', ' ', words1) # Удаление лишних пробелов\n",
    "            words1 = set(words1.strip().split()) # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            words2 = words2.replace('_', ' ') # Удаление _ символов\n",
    "            words2 = ' '.join(re.findall(r'\\b\\d{4}\\b', words2))\n",
    "            words2 = re.sub(r'\\s+', ' ', words2) # Удаление лишних пробелов\n",
    "            words2 = set(words2.strip().split()) # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            four_digits = compute_jac_value(words1, words2)\n",
    "            mfour_digits = compute_jacm_value(words1, words2)\n",
    "            \n",
    "            matches1 = re.findall(r\"[+-]?\\d+(?:[.,]\\d+)?\", name1)\n",
    "            matches2 = re.findall(r\"[+-]?\\d+(?:[.,]\\d+)?\", name2)\n",
    "            #\n",
    "            floats = compute_jac_value(set(matches1), set(matches2))\n",
    "            mfloats = compute_jac_value(set(matches1), set(matches2))\n",
    "            \n",
    "            return n_jac_words, n_jac_ru_words, n_jac_en_words, n_jac_ru_comb_words, n_jac_en_comb_words, n_jac_numbers, num_words, four_digits, floats, n_jacm_words, n_jacm_ru_words, n_jacm_en_words, n_jacm_ru_comb_words, n_jacm_en_comb_words, n_jacm_numbers, mfour_digits, mfloats\n",
    "    \n",
    "\n",
    "        tqdm.pandas(desc='Processing test')\n",
    "        test_df[['n_jac_words', 'n_jac_ru_words', 'n_jac_en_words', 'n_jac_ru_comb_words', 'n_jac_en_comb_words', 'n_jac_numbers', 'n_num_words', 'n_four_digits', 'n_floats', 'n_jacm_words', 'n_jacm_ru_words', 'n_jacm_en_words', 'n_jacm_ru_comb_words', 'n_jacm_en_comb_words', 'n_jacm_numbers', 'n_mfour_digits', 'n_mfloats']] = test_df.progress_apply(lambda row: pd.Series(compute_n_jac(row['variantid1'], row['variantid2'])), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = n_jac(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T06:59:46.714246100Z",
     "start_time": "2024-09-08T06:59:32.120100900Z"
    }
   },
   "id": "41fbb2ba43287d53",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 49620/49620 [00:40<00:00, 1221.46it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('d_jac', False):\n",
    "    def d_jac(test_df, text_df):\n",
    "        \n",
    "        def create_bigrams(sentence):\n",
    "            bigrams = []\n",
    "            for i in range(len(sentence) - 1):\n",
    "                bigrams.append((sentence[i], sentence[i + 1]))\n",
    "            return bigrams\n",
    "        \n",
    "        def filter_bigrams(bigrams):\n",
    "            return [(w1, w2) for (w1, w2) in bigrams if re.match(r'\\d+', w1)]\n",
    "        \n",
    "        def compute_jac_value(tokens1, tokens2):\n",
    "            \n",
    "            if len(tokens1) == 0  or len(tokens2) == 0 :\n",
    "                if len(tokens1) == 0  and len(tokens2) == 0 :\n",
    "                    return -4\n",
    "                else:\n",
    "                    return -3\n",
    "            \n",
    "            intersection = len(tokens1.intersection(tokens2))\n",
    "            union = len(tokens1.union(tokens2))\n",
    "            \n",
    "            return intersection / union if union > 0 else 0\n",
    "        \n",
    "        def compute_jacm_value(tokens1, tokens2):\n",
    "            \n",
    "            if len(tokens1) == 0  or len(tokens2) == 0 :\n",
    "                if len(tokens1) == 0  and len(tokens2) == 0 :\n",
    "                    return -4\n",
    "                else:\n",
    "                    return -3\n",
    "            \n",
    "            intersection = len(tokens1.intersection(tokens2))\n",
    "            # Доля общих элементов относительно множества 1\n",
    "            ratio1 = intersection / len(tokens1)\n",
    "            \n",
    "            # Доля общих элементов относительно множества 2\n",
    "            ratio2 = intersection / len(tokens2)\n",
    "            \n",
    "            # Возвращаем максимальное значение из двух долей\n",
    "            return max(ratio1, ratio2)\n",
    "        \n",
    "        def compute_d_jac(variantid1, variantid2):\n",
    "            \n",
    "            name1 = text_df.at[variantid1, 'description']\n",
    "            name2 = text_df.at[variantid2, 'description']\n",
    "            \n",
    "            if name1 is None:\n",
    "                name1 = ''\n",
    "            if name2 is None:\n",
    "                name2 = ''\n",
    "            \n",
    "            if len(name1) < 3  or len(name2) < 3 :\n",
    "                if len(name1) < 3  and len(name2) < 3 :\n",
    "                    return -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2\n",
    "                else:\n",
    "                    return -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n",
    "                \n",
    "            # Токены без пунктуации\n",
    "            words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            words1 = words1.replace('_', ' ') # Удаление _ символов\n",
    "            words1 = re.sub(r'\\s+', ' ', words1) # Удаление лишних пробелов\n",
    "            words1 = words1.strip().split() # Удаление пробелов в начале и конце    \n",
    "            #\n",
    "            words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            words2 = words2.replace('_', ' ') # Удаление _ символов\n",
    "            words2 = re.sub(r'\\s+', ' ', words2) # Удаление лишних пробелов\n",
    "            words2 = words2.strip().split() # Удаление пробелов в начале и конце    \n",
    "            #\n",
    "            n_jac_words = compute_jac_value(set(words1), set(words2))\n",
    "            n_jacm_words = compute_jacm_value(set(words1), set(words2))\n",
    "            \n",
    "            \n",
    "            ru_words1 = re.sub(r'[^а-яА-Я]', ' ', name1) # Получение токенов с ру символами\n",
    "            ru_words1 = re.sub(r'\\s+', ' ', ru_words1) # Удаление лишних пробелов\n",
    "            ru_words1 = ru_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            ru_words2 = re.sub(r'[^а-яА-Я]', ' ', name2) # Получение токенов с ру символами\n",
    "            ru_words2 = re.sub(r'\\s+', ' ', ru_words2) # Удаление лишних пробелов\n",
    "            ru_words2 = ru_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_ru_words = compute_jac_value(set(ru_words1), set(ru_words2))\n",
    "            n_jacm_ru_words = compute_jacm_value(set(ru_words1), set(ru_words2))\n",
    "            \n",
    "            \n",
    "            en_words1 = re.sub(r'[^a-zA-Z]', ' ', name1) # Получение токенов с ру символами\n",
    "            en_words1 = re.sub(r'\\s+', ' ', en_words1) # Удаление лишних пробелов\n",
    "            en_words1 = en_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            en_words2 = re.sub(r'[^a-zA-Z]', ' ', name2) # Получение токенов с ру символами\n",
    "            en_words2 = re.sub(r'\\s+', ' ', en_words2) # Удаление лишних пробелов\n",
    "            en_words2 = en_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_en_words = compute_jac_value(set(en_words1), set(en_words2))\n",
    "            n_jacm_en_words = compute_jacm_value(set(en_words1), set(en_words2))\n",
    "            \n",
    "            \n",
    "            ru_comb_words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            ru_comb_words1 = ru_comb_words1.replace('_', ' ') # Удаление _ символов\n",
    "            ru_comb_words1 = ' '.join(re.findall(r'\\b(?=\\w*[а-яА-Я])(?=\\w*[^\\Wа-яА-Я])\\w+\\b', ru_comb_words1))\n",
    "            ru_comb_words1 = re.sub(r'\\s+', ' ', ru_comb_words1) # Удаление лишних пробелов\n",
    "            ru_comb_words1 = ru_comb_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            ru_comb_words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            ru_comb_words2 = ru_comb_words2.replace('_', ' ') # Удаление _ символов\n",
    "            ru_comb_words2 = ' '.join(re.findall(r'\\b(?=\\w*[а-яА-Я])(?=\\w*[^\\Wа-яА-Я])\\w+\\b', ru_comb_words2))\n",
    "            ru_comb_words2 = re.sub(r'\\s+', ' ', ru_comb_words2) # Удаление лишних пробелов\n",
    "            ru_comb_words2 = ru_comb_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_ru_comb_words = compute_jac_value(set(ru_comb_words1), set(ru_comb_words2))\n",
    "            n_jacm_ru_comb_words = compute_jacm_value(set(ru_comb_words1), set(ru_comb_words2))\n",
    "            \n",
    "            \n",
    "            en_comb_words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            en_comb_words1 = en_comb_words1.replace('_', ' ') # Удаление _ символов\n",
    "            en_comb_words1 = ' '.join(re.findall(r'\\b(?=\\w*[a-zA-Z])(?=\\w*[^\\Wa-zA-Z])\\w+\\b', en_comb_words1))\n",
    "            en_comb_words1 = re.sub(r'\\s+', ' ', en_comb_words1) # Удаление лишних пробелов\n",
    "            en_comb_words1 = en_comb_words1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            en_comb_words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            en_comb_words2 = en_comb_words2.replace('_', ' ') # Удаление _ символов\n",
    "            en_comb_words2 = ' '.join(re.findall(r'\\b(?=\\w*[a-zA-Z])(?=\\w*[^\\Wa-zA-Z])\\w+\\b', en_comb_words2))\n",
    "            en_comb_words2 = re.sub(r'\\s+', ' ', en_comb_words2) # Удаление лишних пробелов\n",
    "            en_comb_words2 = en_comb_words2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_en_comb_words = compute_jac_value(set(en_comb_words1), set(en_comb_words2))\n",
    "            n_jacm_en_comb_words = compute_jacm_value(set(en_comb_words1), set(en_comb_words2))\n",
    "            \n",
    "            \n",
    "            numbers1 = re.sub(r'[^\\d]', ' ', name1) # Получение токенов с цифрами\n",
    "            numbers1 = re.sub(r'\\s+', ' ', numbers1) # Удаление лишних пробелов\n",
    "            numbers1 = numbers1.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            numbers2 = re.sub(r'[^\\d]', ' ', name2) # Получение токенов с цифрами\n",
    "            numbers2 = re.sub(r'\\s+', ' ', numbers2) # Удаление лишних пробелов\n",
    "            numbers2 = numbers2.strip().split() # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            n_jac_numbers = compute_jac_value(set(numbers1), set(numbers2))\n",
    "            n_jacm_numbers = compute_jacm_value(set(numbers1), set(numbers2))\n",
    "            \n",
    "        \n",
    "            bigrams1 = create_bigrams(words1)\n",
    "            bigrams2 = create_bigrams(words2)\n",
    "            #\n",
    "            filtered_bigrams1 = filter_bigrams(bigrams1)\n",
    "            filtered_bigrams2 = filter_bigrams(bigrams2)\n",
    "            #\n",
    "            length = 0\n",
    "            counter = 0\n",
    "            #\n",
    "            for w1, w2 in filtered_bigrams1:\n",
    "                for x1, x2 in filtered_bigrams2:\n",
    "                    if w2 == x2:\n",
    "                        length += 1\n",
    "                        if w1 == x1:\n",
    "                            counter += 1\n",
    "            #\n",
    "            if length != 0:\n",
    "                num_words = counter / length\n",
    "            else:\n",
    "                num_words = -3\n",
    "            \n",
    "            words1 = re.sub(r'[^\\w\\s]', ' ', name1) # Unicode токены\n",
    "            words1 = words1.replace('_', ' ') # Удаление _ символов\n",
    "            words1 = ' '.join(re.findall(r'\\b\\d{4}\\b', words1))\n",
    "            words1 = re.sub(r'\\s+', ' ', words1) # Удаление лишних пробелов\n",
    "            words1 = set(words1.strip().split()) # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            words2 = re.sub(r'[^\\w\\s]', ' ', name2) # Unicode токены\n",
    "            words2 = words2.replace('_', ' ') # Удаление _ символов\n",
    "            words2 = ' '.join(re.findall(r'\\b\\d{4}\\b', words2))\n",
    "            words2 = re.sub(r'\\s+', ' ', words2) # Удаление лишних пробелов\n",
    "            words2 = set(words2.strip().split()) # Удаление пробелов в начале и конце\n",
    "            #\n",
    "            four_digits = compute_jac_value(words1, words2)\n",
    "            mfour_digits = compute_jacm_value(words1, words2)\n",
    "            \n",
    "            matches1 = re.findall(r\"[+-]?\\d+(?:[.,]\\d+)?\", name1)\n",
    "            matches2 = re.findall(r\"[+-]?\\d+(?:[.,]\\d+)?\", name2)\n",
    "            #\n",
    "            floats = compute_jac_value(set(matches1), set(matches2))\n",
    "            mfloats = compute_jac_value(set(matches1), set(matches2))\n",
    "            \n",
    "            return n_jac_words, n_jac_ru_words, n_jac_en_words, n_jac_ru_comb_words, n_jac_en_comb_words, n_jac_numbers, num_words, four_digits, floats, n_jacm_words, n_jacm_ru_words, n_jacm_en_words, n_jacm_ru_comb_words, n_jacm_en_comb_words, n_jacm_numbers, mfour_digits, mfloats\n",
    "\n",
    "        tqdm.pandas(desc='Processing test')\n",
    "        test_df[['d_jac_words', 'd_jac_ru_words', 'd_jac_en_words', 'd_jac_ru_comb_words', 'd_jac_en_comb_words', 'd_jac_numbers', 'd_num_words', 'd_four_digits', 'd_floats', 'd_jacm_words', 'd_jacm_ru_words', 'd_jacm_en_words', 'd_jacm_ru_comb_words', 'd_jacm_en_comb_words', 'd_jacm_numbers', 'd_mfour_digits', 'd_mfloats']] = test_df.progress_apply(lambda row: pd.Series(compute_d_jac(row['variantid1'], row['variantid2'])), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = d_jac(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:00:27.387705100Z",
     "start_time": "2024-09-08T06:59:46.712154600Z"
    }
   },
   "id": "23d3a254c18b3c7e",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:03<00:00, 13297.22it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('n_lev_opers', False):\n",
    "    def n_lev_opers(test_df, text_df):\n",
    "        \n",
    "        def compute_n_lev_opers(variantid1, variantid2):\n",
    "            \n",
    "            name1 = text_df.at[variantid1, 'name']\n",
    "            name2 = text_df.at[variantid2, 'name']\n",
    "            \n",
    "            if len(name1) < 3  or len(name2) < 3 :\n",
    "                if len(name1) < 3  and len(name2) < 3 :\n",
    "                    return -2, -2\n",
    "                else:\n",
    "                    return -1, -1\n",
    "                \n",
    "            operations = Levenshtein.editops(name1, name2)\n",
    "            \n",
    "            insertions = sum(1 for op in operations if op[0] == 'insert')\n",
    "            deletions = sum(1 for op in operations if op[0] == 'delete')\n",
    "            replaces = sum(1 for op in operations if op[0] == 'replace')\n",
    "            \n",
    "            n_lev_var = insertions + deletions\n",
    "            \n",
    "            max_len = max(len(name1), len(name2))\n",
    "            \n",
    "            return n_lev_var / max_len, replaces / max_len\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df[['n_lev_var', 'n_lev_rep']] = test_df.progress_apply(\n",
    "            lambda row: pd.Series(compute_n_lev_opers(row['variantid1'], row['variantid2'])), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = n_lev_opers(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:00:31.139553Z",
     "start_time": "2024-09-08T07:00:27.384706800Z"
    }
   },
   "id": "7dd6d97507869aba",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:08<00:00, 5699.66it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('d_lev_opers', False):\n",
    "    def d_lev_opers(test_df, text_df):\n",
    "        \n",
    "        def compute_d_lev_opers(variantid1, variantid2):\n",
    "            \n",
    "            desc1 = text_df.at[variantid1, 'description']\n",
    "            desc2 = text_df.at[variantid2, 'description']\n",
    "            \n",
    "            if desc1 is None or desc2 is None:\n",
    "                if desc1 is None and desc2 is None:\n",
    "                    return -2, -2\n",
    "                else:\n",
    "                    return -1, -1\n",
    "            \n",
    "            if len(desc1.replace(' ', '')) == 0  or len(desc2.replace(' ', ''))  == 0:\n",
    "                if len(desc1.replace(' ', '')) == 0  and len(desc2.replace(' ', ''))  == 0:\n",
    "                    return -2, -2\n",
    "                else:\n",
    "                    return -1, -1\n",
    "                \n",
    "            operations = Levenshtein.editops(desc1, desc2)\n",
    "            \n",
    "            insertions = sum(1 for op in operations if op[0] == 'insert')\n",
    "            deletions = sum(1 for op in operations if op[0] == 'delete')\n",
    "            replaces = sum(1 for op in operations if op[0] == 'replace')\n",
    "            \n",
    "            d_lev_var = insertions + deletions\n",
    "            \n",
    "            max_len = max(len(desc1), len(desc2))\n",
    "            \n",
    "            return d_lev_var / max_len, replaces / max_len\n",
    "    \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df[['d_lev_var', 'd_lev_rep']] = test_df.progress_apply(\n",
    "            lambda row: pd.Series(compute_d_lev_opers(row['variantid1'], row['variantid2'])), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = d_lev_opers(test, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:00:39.868305100Z",
     "start_time": "2024-09-08T07:00:31.141552300Z"
    }
   },
   "id": "b48bb1d075bbd986",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:09<00:00, 5362.33it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('m_cos', False):\n",
    "    def m_cos(test_df, resnet_df):\n",
    "        \n",
    "        def compute_m_cos(variantid1, variantid2):\n",
    "            \n",
    "            embedding_1 = resnet_df.at[variantid1, 'main_pic_embeddings_resnet_v1'][0]\n",
    "            embedding_2 = resnet_df.at[variantid2, 'main_pic_embeddings_resnet_v1'][0]\n",
    "            \n",
    "            return cosine_similarity([embedding_1], [embedding_2])[0,0]\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['m_cos'] = test_df.progress_apply(lambda row: compute_m_cos(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "        \n",
    "    test = m_cos(test, resnet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:00:49.125762200Z",
     "start_time": "2024-09-08T07:00:39.860305400Z"
    }
   },
   "id": "551b134ccfcfff3e",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:06<00:00, 7328.69it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('m_evklid', False):\n",
    "    def m_evklid(test_df, resnet_df):\n",
    "        \n",
    "        def compute_m_evklid(variantid1, variantid2):\n",
    "            \n",
    "            embedding_1 = resnet_df.at[variantid1, 'main_pic_embeddings_resnet_v1'][0]\n",
    "            embedding_2 = resnet_df.at[variantid2, 'main_pic_embeddings_resnet_v1'][0]\n",
    "            \n",
    "            return euclidean_distances([embedding_1], [embedding_2])[0, 0]\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['m_evklid'] = test_df.progress_apply(lambda row: compute_m_evklid(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "        \n",
    "    test = m_evklid(test, resnet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:00:55.908317100Z",
     "start_time": "2024-09-08T07:00:49.127762600Z"
    }
   },
   "id": "d483d75d553e1a89",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:07<00:00, 6823.65it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('e_jac', False):\n",
    "    def e_jac(test_df, resnet_df):\n",
    "        \n",
    "        def calculate_e_jac(v1, v2):\n",
    "            embs1 = resnet_df.at[v1, 'pic_embeddings_resnet_v1']\n",
    "            embs2 = resnet_df.at[v2, 'pic_embeddings_resnet_v1'] \n",
    "            \n",
    "            if embs1 is None or embs2 is None:\n",
    "                if embs1 is None and embs2 is None:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            set1 = set(tuple(emb) for emb in embs1)\n",
    "            set2 = set(tuple(emb) for emb in embs2)\n",
    "            \n",
    "            intersection = set1.intersection(set2)\n",
    "            union = set1.union(set2)\n",
    "            \n",
    "            if not union:\n",
    "                return 0\n",
    "\n",
    "            return len(intersection) / len(union)\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['e_jac'] = test_df.progress_apply(lambda row: calculate_e_jac(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "        \n",
    "    test = e_jac(test, resnet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:01:03.191306400Z",
     "start_time": "2024-09-08T07:00:55.907317300Z"
    }
   },
   "id": "aa429a0fdfdd7252",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:00<00:00, 88332.13it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('e_diff', False):\n",
    "    def e_diff(test_df, resnet_df):\n",
    "        \n",
    "        def count_e_diff(variantid1, variantid2):\n",
    "            emb1 = resnet_df.at[variantid1, 'pic_embeddings_resnet_v1']\n",
    "            emb2 = resnet_df.at[variantid2, 'pic_embeddings_resnet_v1']\n",
    "            \n",
    "            if emb1 is None or emb2 is None:\n",
    "                if emb1 is None and emb2 is None:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            return abs(len(emb1) - len(emb2)) / max(len(emb1), len(emb2))\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['e_diff'] = test_df.progress_apply(lambda row: count_e_diff(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "        \n",
    "    test = e_diff(test, resnet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:01:03.774754300Z",
     "start_time": "2024-09-08T07:01:03.191306400Z"
    }
   },
   "id": "c544928cc5bf790c",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:14<00:00, 3394.99it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('m_ent_diff', False):\n",
    "    def m_ent_diff(test_df, resnet_df):\n",
    "        \n",
    "        def compute_entropy_diff(embedding):\n",
    "            \n",
    "            norm_embedding = np.abs(embedding) / np.sum(np.abs(embedding))\n",
    "            \n",
    "            return entropy(norm_embedding)\n",
    "    \n",
    "        def compute_m_ent_diff(variantid1, variantid2):\n",
    "            embedding1 = resnet_df.at[variantid1, 'main_pic_embeddings_resnet_v1'][0]\n",
    "            embedding2 = resnet_df.at[variantid2, 'main_pic_embeddings_resnet_v1'][0]\n",
    "    \n",
    "            entropy1 = compute_entropy_diff(embedding1)\n",
    "            entropy2 = compute_entropy_diff(embedding2)\n",
    "            \n",
    "            return abs(entropy1 - entropy2)\n",
    "        \n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['m_ent_diff'] = test_df.progress_apply(lambda row: compute_m_ent_diff(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "    \n",
    "    test = m_ent_diff(test, resnet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:01:18.403456200Z",
     "start_time": "2024-09-08T07:01:03.766737900Z"
    }
   },
   "id": "b1e013a2285b9230",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:07<00:00, 7077.00it/s]\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.get('e_avg_cos', False):\n",
    "    def e_avg_cos(test_df, resnet_df):\n",
    "        \n",
    "        def average_embedding(embeddings):\n",
    "            return np.mean(np.array(embeddings, copy=False).copy(), axis=0)\n",
    "\n",
    "\n",
    "        def calculate_e_avg_cos(v1, v2):\n",
    "            emb1 = resnet_df.at[v1, 'pic_embeddings_resnet_v1']\n",
    "            emb2 = resnet_df.at[v2, 'pic_embeddings_resnet_v1'] \n",
    "            \n",
    "            if emb1 is None or emb2 is None:\n",
    "                if emb1 is None and emb2 is None:\n",
    "                    return -2\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "            return cosine_similarity([average_embedding(emb1)], [average_embedding(emb2)])[0, 0]\n",
    "\n",
    "        tqdm.pandas(desc=\"Processing test_df\")\n",
    "        test_df['e_avg_cos'] = test_df.progress_apply(lambda row: calculate_e_avg_cos(row['variantid1'], row['variantid2']), axis=1)\n",
    "        \n",
    "        return test_df\n",
    "        \n",
    "    test = e_avg_cos(test, resnet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:01:25.431252400Z",
     "start_time": "2024-09-08T07:01:18.398459200Z"
    }
   },
   "id": "17a774061980dbf6",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('my_data/top_attributes_per_category.pkl', 'rb') as f:\n",
    "    top_attributes_per_category = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:01:26.242191700Z",
     "start_time": "2024-09-08T07:01:26.214177Z"
    }
   },
   "id": "429ea073d58faaf2",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_df: 100%|██████████| 49620/49620 [00:03<00:00, 16346.04it/s]\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_11588\\357030163.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n"
     ]
    }
   ],
   "source": [
    "n_samples = 300\n",
    "def attrs300(test_df, attributes_df):\n",
    "        \n",
    "    # Функция для вычисления значений атрибутов\n",
    "    def compute_attrs300(variantid1, variantid2, n_samples=n_samples):\n",
    "        attrs1 = json.loads(attributes_df.at[variantid1, 'characteristic_attributes_mapping'])\n",
    "        attrs2 = json.loads(attributes_df.at[variantid2, 'characteristic_attributes_mapping'])\n",
    "        \n",
    "        cat1 = json.loads(attributes_df.at[variantid1, 'categories'])['2']\n",
    "        \n",
    "        # Получение 10 наиболее частых атрибутов для данной категории\n",
    "        top_attributes = top_attributes_per_category.get(cat1, [])\n",
    "        \n",
    "        # Инициализация результатов\n",
    "        result = [-2] * n_samples # Значение по умолчанию для отсутствующих атрибутов\n",
    "        \n",
    "        # Сравнение значений атрибутов\n",
    "        for i, attr in enumerate(top_attributes):\n",
    "            if attr in attrs1 or attr in attrs2:\n",
    "                if attr in attrs1 and attr in attrs2:\n",
    "                    vals1 = attrs1[attr]\n",
    "                    vals2 = attrs2[attr]\n",
    "                    if len(vals1) == 1 and len(vals2) == 1:\n",
    "                        if bool(re.match(r'^[-+]?\\d*\\.?\\d+([eE][-+]?\\d+)?$', attrs1[attr][0])) and bool(re.match(r'^[-+]?\\d*\\.?\\d+([eE][-+]?\\d+)?$', attrs2[attr][0])):\n",
    "                            if vals1 == vals2:\n",
    "                                result[i] = 1 \n",
    "                            else:\n",
    "                                val1 = float(vals1[0])\n",
    "                                val2 = float(vals2[0])\n",
    "                                 # Используем абсолютное значение разности\n",
    "                                abs_diff = abs(val1 - val2)\n",
    "                                # Нормируем разницу, чтобы она находилась в диапазоне от 0 до 1\n",
    "                                # В этом случае устанавливаем максимальное значение для нормализации.\n",
    "                                max_diff = max(abs(val1), abs(val2))\n",
    "                                result[i] = 1 - (abs_diff / (max_diff if max_diff != 0 else 1))\n",
    "                                \n",
    "                    else:\n",
    "                        val1 = ' '.join(vals1)\n",
    "                        val2 = ' '.join(vals2)\n",
    "                        \n",
    "                        # Токены без пунктуации\n",
    "                        words1 = re.sub(r'[^\\w\\s]', ' ', val1) # Unicode токены\n",
    "                        words1 = words1.replace('_', ' ') # Удаление _ символов\n",
    "                        words1 = re.sub(r'\\s+', ' ', words1) # Удаление лишних пробелов\n",
    "                        tokens1 = set(words1.strip().split()) # Удаление пробелов в начале и конце    \n",
    "                        #\n",
    "                        words2 = re.sub(r'[^\\w\\s]', ' ', val2) # Unicode токены\n",
    "                        words2 = words2.replace('_', ' ') # Удаление _ символов\n",
    "                        words2 = re.sub(r'\\s+', ' ', words2) # Удаление лишних пробелов\n",
    "                        tokens2 = set(words2.strip().split()) # Удаление пробелов в начале и конце    \n",
    "                        \n",
    "                        if len(tokens1) == 0  or len(tokens2) == 0 :\n",
    "                            if len(tokens1) == 0 and len(tokens2) == 0:\n",
    "                                result[i] = -4\n",
    "                            else:\n",
    "                                result[i] = -3\n",
    "                        \n",
    "                        intersection = len(tokens1.intersection(tokens2))\n",
    "                        union = len(tokens1.union(tokens2))\n",
    "                        \n",
    "                        result[i] = intersection / union if union > 0 else 0\n",
    "                else:\n",
    "                    result[i] = -1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    tqdm.pandas(desc=\"Processing test_df\")\n",
    "    test_df[[f'attr{i}' for i in range(1, n_samples + 1)]] = pd.DataFrame(\n",
    "        test_df.progress_apply(lambda row: compute_attrs300(row['variantid1'], row['variantid2']), axis=1).tolist(),\n",
    "        index=test_df.index\n",
    "    )\n",
    "\n",
    "    return test_df\n",
    "\n",
    "# Применение функции к данным\n",
    "test = attrs300(test, attributes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:01:32.123524Z",
     "start_time": "2024-09-08T07:01:26.902670600Z"
    }
   },
   "id": "eb661d1b386bc8a7",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_hgb = joblib.load('final_hgb_model.pkl')\n",
    "hgb_preds = final_hgb.predict_proba(test.drop(columns=['variantid1', 'variantid2']))[:, 1]\n",
    "submission = pd.DataFrame({\n",
    "    'variantid1': test['variantid1'],\n",
    "    'variantid2': test['variantid2'],\n",
    "    'target': hgb_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T07:01:33.823929300Z",
     "start_time": "2024-09-08T07:01:32.121459800Z"
    }
   },
   "id": "38262ecfcc1395c3",
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
